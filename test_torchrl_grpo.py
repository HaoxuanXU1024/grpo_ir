#!/usr/bin/env python3
"""
æµ‹è¯•TorchRL GRPOå®ç°çš„è„šæœ¬
éªŒè¯ç¯å¢ƒã€ç­–ç•¥ç½‘ç»œå’Œè®­ç»ƒå¾ªç¯æ˜¯å¦æ­£å¸¸å·¥ä½œ
"""

import torch
import numpy as np
from tensordict import TensorDict

from grpo_torchrl_env import AdaIRGRPOEnv, AdaIRPolicyNetwork
from train_grpo_torchrl import SimplifiedPPOLoss, TorchRLGRPOTrainer
from net.model import AdaIR


def test_environment():
    """æµ‹è¯•ç¯å¢ƒæ˜¯å¦æ­£å¸¸å·¥ä½œ"""
    print("=== æµ‹è¯•AdaIR GRPOç¯å¢ƒ ===")
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    batch_size = 2
    
    # åˆ›å»ºç¯å¢ƒ
    env = AdaIRGRPOEnv(batch_size=batch_size, device=device)
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    degraded = torch.randn(batch_size, 3, 128, 128, device=device)
    clean = torch.randn(batch_size, 3, 128, 128, device=device)
    degraded = torch.clamp(degraded, 0, 1)
    clean = torch.clamp(clean, 0, 1)
    
    # è®¾ç½®ç¯å¢ƒæ•°æ®
    env.set_data(degraded, clean)
    
    # åˆ›å»ºæµ‹è¯•åŠ¨ä½œ
    freq_params = torch.randn(batch_size, 3, 8, device=device) * 0.1 + 1.0
    action_dict = {
        "action": {
            "freq_params": freq_params
        }
    }
    
    # æµ‹è¯•ç¯å¢ƒæ­¥è¿›
    result = env.step(action_dict)
    
    print(f"âœ… ç¯å¢ƒæµ‹è¯•æˆåŠŸ!")
    print(f"  å¥–åŠ±å½¢çŠ¶: {result['reward'].shape}")
    print(f"  å¥–åŠ±å€¼: {result['reward'].mean().item():.4f}")
    print(f"  å®ŒæˆçŠ¶æ€: {result['done'].all()}")
    
    return True


def test_policy_network():
    """æµ‹è¯•ç­–ç•¥ç½‘ç»œ"""
    print("\n=== æµ‹è¯•ç­–ç•¥ç½‘ç»œ ===")
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    batch_size = 2
    
    # åˆ›å»ºç­–ç•¥ç½‘ç»œ
    policy = AdaIRPolicyNetwork().to(device)
    
    # åˆ›å»ºæµ‹è¯•è§‚å¯Ÿ
    obs = TensorDict({
        "degraded_image": torch.randn(batch_size, 3, 128, 128, device=device),
        "clean_image": torch.randn(batch_size, 3, 128, 128, device=device),
    }, batch_size=(batch_size,), device=device)
    
    # å‰å‘ä¼ æ’­
    action = policy(obs)
    
    print(f"âœ… ç­–ç•¥ç½‘ç»œæµ‹è¯•æˆåŠŸ!")
    print(f"  åŠ¨ä½œå‚æ•°å½¢çŠ¶: {action['action']['freq_params'].shape}")
    print(f"  å‚æ•°èŒƒå›´: {action['action']['freq_params'].min().item():.3f} - {action['action']['freq_params'].max().item():.3f}")
    
    return True


def test_ppo_loss():
    """æµ‹è¯•PPOæŸå¤±å‡½æ•°"""
    print("\n=== æµ‹è¯•PPOæŸå¤±å‡½æ•° ===")
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    batch_size = 4
    
    # åˆ›å»ºPPOæŸå¤±
    ppo_loss = SimplifiedPPOLoss()
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    log_probs = torch.randn(batch_size, device=device)
    old_log_probs = torch.randn(batch_size, device=device)
    advantages = torch.randn(batch_size, device=device)
    returns = torch.randn(batch_size, device=device)
    values = torch.randn(batch_size, device=device)
    
    # è®¡ç®—æŸå¤±
    loss_dict = ppo_loss(log_probs, old_log_probs, advantages, returns, values)
    
    print(f"âœ… PPOæŸå¤±æµ‹è¯•æˆåŠŸ!")
    print(f"  æ€»æŸå¤±: {loss_dict['loss_total'].item():.4f}")
    print(f"  ç­–ç•¥æŸå¤±: {loss_dict['loss_policy'].item():.4f}")
    print(f"  ä»·å€¼æŸå¤±: {loss_dict['loss_value'].item():.4f}")
    print(f"  ç†µæŸå¤±: {loss_dict['loss_entropy'].item():.4f}")
    
    return True


def test_full_training_step():
    """æµ‹è¯•å®Œæ•´çš„è®­ç»ƒæ­¥éª¤"""
    print("\n=== æµ‹è¯•å®Œæ•´è®­ç»ƒæ­¥éª¤ ===")
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    batch_size = 1
    
    # åˆ›å»ºç¯å¢ƒå’Œç½‘ç»œ
    env = AdaIRGRPOEnv(batch_size=batch_size, device=device)
    policy = AdaIRPolicyNetwork().to(device)
    value_net = torch.nn.Sequential(
        torch.nn.Conv2d(3, 64, 3, padding=1),
        torch.nn.ReLU(),
        torch.nn.AdaptiveAvgPool2d((8, 8)),
        torch.nn.Flatten(),
        torch.nn.Linear(64 * 8 * 8, 512),
        torch.nn.ReLU(),
        torch.nn.Linear(512, 1),
    ).to(device)
    
    ppo_loss = SimplifiedPPOLoss()
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    degraded = torch.randn(batch_size, 3, 128, 128, device=device)
    clean = torch.randn(batch_size, 3, 128, 128, device=device)
    degraded = torch.clamp(degraded, 0, 1)
    clean = torch.clamp(clean, 0, 1)
    
    # è®¾ç½®ç¯å¢ƒ
    env.set_data(degraded, clean)
    
    # åˆ›å»ºè§‚å¯Ÿ
    obs = TensorDict({
        "degraded_image": degraded,
        "clean_image": clean,
    }, batch_size=(batch_size,), device=device)
    
    # ç­–ç•¥é‡‡æ ·
    action = policy(obs)
    
    # ç¯å¢ƒæ­¥è¿›
    env_input = TensorDict({
        "degraded_image": degraded,
        "clean_image": clean,
        "action": action["action"]
    }, batch_size=(batch_size,), device=device)
    
    result = env.step(env_input)
    
    # ä»·å€¼ä¼°è®¡
    values = value_net(degraded).squeeze(-1)
    
    # è®¡ç®—ç®€åŒ–çš„ä¼˜åŠ¿å’Œå›æŠ¥
    rewards = result["reward"].squeeze(-1)
    advantages = rewards - values.detach()
    returns = rewards
    
    # logæ¦‚ç‡ï¼ˆç®€åŒ–ï¼‰
    log_probs = -0.5 * torch.sum(action["action"]["freq_params"] ** 2, dim=(1, 2))
    
    # è®¡ç®—æŸå¤±
    loss_dict = ppo_loss(log_probs, log_probs.detach(), advantages, returns, values)
    
    print(f"âœ… å®Œæ•´è®­ç»ƒæ­¥éª¤æµ‹è¯•æˆåŠŸ!")
    print(f"  å¥–åŠ±: {rewards.mean().item():.4f}")
    print(f"  ä¼˜åŠ¿: {advantages.mean().item():.4f}")
    print(f"  æ€»æŸå¤±: {loss_dict['loss_total'].item():.4f}")
    
    return True


def test_adair_model():
    """æµ‹è¯•AdaIRæ¨¡å‹åŠ è½½"""
    print("\n=== æµ‹è¯•AdaIRæ¨¡å‹ ===")
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    
    # åˆ›å»ºAdaIRæ¨¡å‹
    model = AdaIR(decoder=True).to(device)
    model.eval()
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    test_input = torch.randn(1, 3, 256, 256, device=device)
    
    with torch.no_grad():
        output = model(test_input)
    
    print(f"âœ… AdaIRæ¨¡å‹æµ‹è¯•æˆåŠŸ!")
    print(f"  è¾“å…¥å½¢çŠ¶: {test_input.shape}")
    print(f"  è¾“å‡ºå½¢çŠ¶: {output.shape}")
    print(f"  è¾“å‡ºèŒƒå›´: {output.min().item():.3f} - {output.max().item():.3f}")
    
    return True


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¼€å§‹æµ‹è¯•TorchRL GRPOå®ç°...")
    
    tests = [
        ("AdaIRæ¨¡å‹", test_adair_model),
        ("ç¯å¢ƒ", test_environment),
        ("ç­–ç•¥ç½‘ç»œ", test_policy_network),
        ("PPOæŸå¤±", test_ppo_loss),
        ("å®Œæ•´è®­ç»ƒæ­¥éª¤", test_full_training_step),
    ]
    
    passed = 0
    total = len(tests)
    
    for test_name, test_func in tests:
        try:
            if test_func():
                passed += 1
            else:
                print(f"âŒ {test_name}æµ‹è¯•å¤±è´¥")
        except Exception as e:
            print(f"âŒ {test_name}æµ‹è¯•å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
    
    print(f"\nğŸ“Š æµ‹è¯•ç»“æœ: {passed}/{total} é€šè¿‡")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡! TorchRL GRPOå®ç°å°±ç»ª!")
        print("\nä¸‹ä¸€æ­¥å¯ä»¥è¿è¡Œ:")
        print("  bash run_torchrl_grpo.sh")
    else:
        print("âš ï¸  æœ‰æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥å®ç°")
    
    return passed == total


if __name__ == "__main__":
    main() 